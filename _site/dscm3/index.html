<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="/site/css/main.css">


    <link rel="shortcut icon" href="/site/assets/favicon.ico" type="image/x-icon">
<link rel="icon" href="/site/assets/favicon.ico" type="image/x-icon">
<meta name="referrer" content="no-referrer">

</head>


  <body>

    <main class="wrapper">
      
      <header class="site-header">
  <a class="site-title" href="/site/"><img src="/site/assets/DSClogo.png" style="width:829px;height:288px;"></a>

  <nav class="navbar">
    <ul>
      <li class="nav-item"><a href="/site/">Home</a></li>
      <li class="nav-item"><a href="/site/chapter-2/">Chapter 2</a></li>
      <li class="nav-item"><a href="/site/books/">Books</a></li>
      <li class="nav-item"><a href="/site/dhawards2019/">DH Awards 2019</a></li>
	  <li class="nav-item"><a href="/site/covid19/">COVID-19</a></li>
    </ul>
  </nav>

</header>


      <div class="container">
        <h1 id="dsc-multilingual-mystery-2-quinn-and-lee-clean-up-ghost-cat-data-hairballs">DSC Multilingual Mystery #2: Quinn and Lee Clean Up Ghost Cat Data-Hairballs</h1>

<p>by Lee Skallerup Bessette and Quinn Dombrowski, April 2, 2020</p>

<p>Dear Reader,</p>

<p>Weâ€™ve been writing this Multilingual Mystery on and off for the last month. During that time, Lee performed non-stop heroics for the emergency shift to online instruction at Georgetown. Quinn finished teaching her Dungeons-and-Dragons style DH role-playing game course virtually, watching as COVID-19 disrupted the lives of the characters â€“ and the students. For more than two weeks now, the Bay Area has been on lockdown, and Quinn has been running a preschool/kindergarten for three kids 6-and-under while trying to help non-English language and literature faculty at Stanford figure out what to do for spring quarter. Lee is also <a href="https://readywriting.org/working-from-home/">working from home with older kids</a>, and weâ€™re both grappling with how to <a href="http://quinndombrowski.com/blog/2020/03/27/pandemic-parenting-pedagogy">manage what the school districts are putting together for K-12 virtual education</a>.</p>

<p>Throughout all of this, we kept coming back to writing this Data-Sitters Club â€œbookâ€. Weâ€™ve had to go back and edit it in places (ballet classes are no longer happening), and weâ€™ve felt moments of estrangement from words written less than a month before. Thereâ€™s been nights when weâ€™ve been <a href="http://quinndombrowski.com/blog/2020/03/21/working-conditions">too exhausted to do anything else</a>, but this project has been a reassuring distraction. The topics â€“ data downloading, web scraping, and data cleaning â€“ may seem dry and technical, but in this strange time thereâ€™s something comforting about writing up how to have control over <em>something</em>â€¦ even if itâ€™s just text on a screen.</p>

<p>We hope it brings you some comfort, too. If you can put it to use in some way that helps other people, so much the better â€“ whether itâ€™s gathering data about hiring freezes, dubious government statements, or 80â€™s/90â€™s cultural phenomena that might bring a smile to someoneâ€™s face in a dark time. And if youâ€™re not in the mental space right now to tackle it, thatâ€™s okay too. Itâ€™ll be here for you when the time is right.</p>

<p>Take care,</p>

<p>Quinn &amp; Lee</p>

<p>P.S. If you want a laugh, check out our ongoing series of <a href="http://datasittersclub.github.io/site/covid19">Important Public Health Messages from the Data-Sitters Club</a>.</p>

<h2 id="recap">Recap</h2>

<p>When we left off with <a href="https://datasittersclub.github.io/site/dscm2/">DSC Multilingual Mystery #2</a>, we had realized that metadata is going to be essential for just about anything we do with computational text analysis at scale for these translations. And in DSC #4 (forthcoming soon!), Anouk comes to the same conclusions about the original corpus in English. It was time to investigate metadata acquisition and cleaning, and the DSC Multilingual Mystery crew was on the case!</p>

<h2 id="lee">Lee</h2>
<p>As I mentioned in the <a href="https://datasittersclub.github.io/site/dscm1/">DSC Multilingual Mystery #1</a>, I already had a pretty good working idea about where to get the metadata we needed to be able to identify translators and other publication data for the various foreign-language translations of the series: national libraries. And, I wasnâ€™t wrong. The <a href="https://banq.qc.ca/accueil/">BibliothÃ¨que et Archives nationales du QuÃ©bec</a>, the <a href="https://www.bnf.fr/fr">BibliothÃ¨que nationale de France</a>, the <a href="http://www.bne.es/es/Inicio/index.html">Biblioteca Nacional de EspaÃ±a</a>, the <a href="https://www.bncf.firenze.sbn.it/">Biblioteca Nazionale Centrale di Firenze</a>, and the <a href="https://www.kb.nl/en">Koninklijke Bibliotheek</a> provided me with all of the information we would need to get the information we required. Oh, and the five lonely German translations from <a href="https://www.dnb.de/EN/Home/home_node.html">Deutsche Nationalbibliothek</a>.</p>

<p>My methodology was simple: Google the country name and â€œnational library.â€ Put Ann M. Martin in the search box (thank goodness for the universal language of user experience and expectations). If that didnâ€™t work, find the word that most closely resembles â€œcatalogueâ€ and click on it to find a different search box. And then, the final and least simple of the steps: export the metadata.</p>

<p>This is where the inconsistencies arose, with each library offering its own idiosyncratic way of getting the metadata out of the library catalogue and into the hands of users. Quebec would only let me export a certain number of results from my search at a time as a csv file. I decided to limit my search by time period, as it was easiest to limit my searches in the interface that way. So, there are I had one CSV file for each year that the books were being published. France was the easiest to export, allowing me to download my entire search (which included the France and Belgium translations) as a giant CSV file. Spain let me export a page of results at a time as XML that displayed in my browser window, which I copied into a file. Shout-out to the Netherlands for having a functional English interfaceâ€¦and exporting the search as a simple text file. And then Italyâ€¦Italy made me cut and paste every single entry individually as XML as there was no batch export function.</p>

<p>All I gotta say is that yâ€™all are lucky I love to solve a good mystery and that I only have the mental capacity for menial tasks while waiting during my daughterâ€™s ballet class where I found myself sitting in the most uncomfortable seat imaginable while seven different dance studios practice five different dance styles, playing seven different songs repeatedly. Sigh. I never thought Iâ€™d say this, but I miss those days.</p>

<p>There was often the option to export the results as a simple text file, but the challenge was that simple text often didnâ€™t preserve the accents on letters, becoming gibberish instead, making it difficult to understand the information and data. So, CSV and XML were the closest I could get to relatively universal readability, or at least the cleanest data I could get for Quinn. It wasnâ€™t perfect, but it was a good first start. Also, the metadata you extract is only as good as the metadata contained in the bookâ€™s library entry, and apparently no one bothered in either France or Belgium to note who translated the first handful of Belgian-French translations of the books. So while I extracted all of the data that was available, there are still some mysteries that might only be solved by either looking at a physical copy of the books or reaching out to the publisher to see if they have preserved the archives or kept the records somewhere. More mysteries that maybe weâ€™ll be able to someday solve if weâ€™re ever allowed to leave our houses again (lol*sob).</p>

<p>I was also curious to see if there were other translations in common Romance languages, so I looked in national libraries in South America for other Spanish translations, as well as Brazil and Portugal for Portugese translations. I came up empty-handed, but it was worth it to look to see if there were any other translations in a language that I could recognize. Which brings up the next limitation in my search: I know English and French fluently, have six credits of university Spanish, and feel pretty comfortable given my knowledge of French and Spanish in navigating an Italian language library website. So while there are more translations, I didnâ€™t feel like I was comfortable enough to rummage around in the national libraries of other languages.</p>

<h2 id="quinn">Quinn</h2>
<p>I was amazed at what Lee managed to find in those national library catalog records. There it was: the answers to our question about the number of translations, and who most of the translators were! One beautiful thing about most national libraries, many museums, and various other online digital collections is that thereâ€™s often an option for just downloading your search or browsing results, which saves you a <em>ton of work</em> relative to trying to do web scraping to collect the same information. Before you embark on web scraping from any cultural heritage website, be sure to read the documentation and FAQs, Google around, and be extra, extra sure that thereâ€™s not an easier route to get the data directly. Iâ€™m not kidding: about half my â€œweb scrapingâ€ consults at work have been resolved by me finding some kind of â€œdownloadâ€ button. Downloading the data isnâ€™t everything, though: I knew that if we didnâ€™t spend the time now to reorganize it into a consistent format that we could easily use to look things up and check for correlations (e.g. between translators and certain character-name inconsistencies), weâ€™d definitely regret it later.</p>

<p>Now, itâ€™s easy to go overboard with data cleaning. Sometimes my library degree gets the better of me, and I start organizing All The Things. (This in contrast to my usual inclination to just pile things in places and assume itâ€™ll all sort itself out when the time is right.) With DH in particular, thereâ€™s a temptation to clean more than you need, and produce a beautiful data set that has all the information perfectly structured. Itâ€™s usually driven by altruism towards your future self or other scholars. â€œSomeday, <em>someone might want this.</em>â€ And if the data is easy and quick to clean, thereâ€™s not much lost in that investment. But if itâ€™s a gnarly problem, and your future use case is hypothetical, maybe itâ€™s worth reconsidering whether itâ€™s the best use of your time <em>right now</em>. If someone wants a clean version of that data in the future, itâ€™s okay to let it be their problem. In the national library records that Lee found, thereâ€™s a lot of data like that. Would it be interesting to see the publication cities for all the translations? Sure! Would it be fun to look at differences in subject metadata? Absolutely! (Iâ€™m really tempted by that one, truth be told.) But the data for both of those is kind of annoying to clean, and not actually what we need to answer the questions weâ€™re working on. Sometimes the hardest part of DH is setting aside all the things you <em>could do</em>, to finish the thing that youâ€™re <em>actually doing</em>.</p>

<p>What I was <em>actually doing</em> was trying to get five pieces of information out of the data that Lee downloaded, for each book:</p>

<ol>
  <li>The translated title of the book</li>
  <li>A unique identifier to connect back to the original book (book number, title, etc.)</li>
  <li>Which translation series is appears in (e.g. for French: Quebec, Belgium, or France)</li>
  <li>The date it was published</li>
  <li>The translator</li>
</ol>

<p>Anything else in these files â€“ regardless of how many great ideas I had for what I might do with it â€“ was a distraction.</p>

<p>But wait! We already started wondering about whether certain things we were finding were quirks of particular ghostwriters. This was the perfect time to scrape and clean a data set that could help us answer those kinds of questions, too. So before we get to cleaning, letâ€™s see what else the Ghost Cat of Data-Sitters Club Multilingual Mystery #3 might grace us with, all over the carpet! (Sorry, readers â€“ what can I say, weâ€™re <a href="https://babysittersclub.fandom.com/wiki/Mallory_and_the_Ghost_Cat">committed to fidelity to the original book titles</a>, however tortured it gets.)</p>

<h3 id="web-scraping">Web scraping</h3>

<p>I do a lot of web scraping. When I need to do something simple, quick, and relatively small-scale, I go with webscraper.io, even though it gives you less flexibility in structuring and exporting your results compared to Python. I knew the <a href="https://babysittersclub.fandom.com/wiki/The_Baby-Sitters_Club_Wiki">Baby-Sitters Club Wiki</a> on Fandom.com had the data I needed, presented as well-structured metadata on each book page. Webscraper.io was going to be a good tool for this job.</p>

<p>Webscraper.io is a plugin for the Chrome browser, so first you need to <a href="https://webscraper.io/documentation/installation">install it from the Chrome store</a>. Next, you need to 1) access it in your browser by opening the <em>Developer Tools</em> panel, then 2) choose <em>Web Scraper</em> from the tabs at the top of the panel.</p>

<p><img src="/site/assets/dscm3_launchwebscraper.jpg" alt="Launching the webscraper plugin" /></p>

<h4 id="creating-a-sitemap">Creating a sitemap</h4>

<p>Using Webscraper.ioâ€™s menu, go to <em>Create new sitemap &gt; Create sitemap</em>, as shown by arrow 3, above. (Note: if you want to import an existing sitemap, like one of the complete ones that weâ€™ve posted at the <a href="https://github.com/datasittersclub/dscm3">Data-Sitters Club GitHub repo for this book</a>, you can choose â€œImport Sitemapâ€ and copy and paste the text from the appropriate sitemap text file into the text field. If you run into trouble with the instructions here, importing one of our sitemaps and playing around with it might help you understand it better.)</p>

<p>The first page is where you put in the start URL of the page you want to scrape. If youâ€™re trying to scrape multiple pages of data, and the site uses pagination that involves appending some page number to the end of the URL (e.g. if you go to the second page of results and you see the URL is the same except for something like <code class="highlighter-rouge">?page=2</code> on the end), you can set a range here, e.g. http://www.somesite.com/results?page=[1-5] if there are 5 pages of results. In our case, though, we have eight <em>different</em> web pages we want to scrape for URLs, one for each book (sub-)series.</p>

<p>First, weâ€™ll give our scraper a name (it can be almost anything; I went with <em>bsc_fan_wiki_link_scraper</em>). For the URL, I just put in the URL for the main book series, <a href="https://babysittersclub.fandom.com/wiki/Category:The_Baby-Sitters_Club_series">https://babysittersclub.fandom.com/wiki/Category:The_Baby-Sitters_Club_series</a>. If you have multiple pages (and especially if youâ€™re putting in a range), itâ€™s often best to start by putting in a single page, setting up the scraper, checking the results, and seeing if you need to make any modifications before you scrape hundreds of pages without capturing what you need. (Trust me â€“ itâ€™s a mistake Iâ€™ve made!)</p>

<h4 id="handling-selectors-easy-mode">Handling selectors: easy mode</h4>

<p>Web scraping is <em>a lot easier</em> if you know at least a little bit about HTML, how itâ€™s structured, and some common elements. Especially once you get into Python-based web scraping, CSS becomes important, too. I was lucky enough to learn HTML in elementary school (my final project in the 5th grade was a Sailor Moon fan site, complete with starry page background and at least one <blink> element), so it's hard for me to remember not knowing this stuff. But there's lots of tutorials out there, like this one from [W3Schools](https://www.w3schools.com/html/html_intro.asp), that can get you up to speed with the basics and let you play around with it if you're not really comfortable with HTML.</blink></p>

<p>The way we set up a web scraper is related to the way HTML is structured: HTML is made up of nested elements, and if weâ€™re doing something complex with our scraper, itâ€™ll have a nested structure, too.</p>

<p>For our first scraper, weâ€™re <em>just</em> trying to get the URLs of all the links on each page.</p>

<p>Weâ€™ll start by hitting the â€œAdd new selectorâ€ button, which takes us to a different interface for choosing the selector. You have to give it a unique ID (I chose <em>pagelink</em>), then choose a type from the dropdown menu. For now, we just want <em>Link</em> for the type. Under the <em>Selector</em> header in this interface, check the â€œmultipleâ€ box (there are multiple links on the page), then click the â€œSelectâ€ button. Now the fun begins! Move your mouse back up to the page, and start clicking on the things you want to capture. After youâ€™ve clicked on 2-3 of them, the scraper usually gets the idea and highlights the rest. In this case, I started clicking in the â€œAâ€™sâ€, and after two clicks it picked up the rest of the entries filed under that letter, but I had to click in the results for a different letter before it picked up <em>everything</em>.</p>

<p><img src="/site/assets/dscm3_selecting_links.jpg" alt="Selecting links for the scraper" /></p>

<p>When everything you want is highlighted in red, click the blue â€œDone selectingâ€ button. What I then had was <code class="highlighter-rouge">ul:nth-of-type(n+3) a.category-page__member-link</code>. If youâ€™re not comfortable with HTML, you might just be relieved that the computer sorted out all <em>that</em> mess. But if you know how to read this, you should be concerned. <code class="highlighter-rouge">&lt;ul&gt;</code> means â€œunordered listâ€ (typically, but not always, looks like a bullet point list), and weâ€™re grabbing the 3rd &gt;ul&lt; on the page. Which, yes, is what we selected: by starting with A on the <a href="https://babysittersclub.fandom.com/wiki/Category:The_Baby-Sitters_Club_series">regular book series page</a>, weâ€™re skipping the list with character-based category pages, and the erroneous list where a wiki page name starts with the number 3. If we were just scraping this one page, itâ€™d be fine: weâ€™d get what we selected, which â€“ deliberately â€“ is not <em>every link on the page</em>. But what about the Super Specials, Mysteries, Super Mysteries, etc? If we start with the third list, will we be missing things?</p>

<p>Take it from someone whoâ€™s had to scrape (and re-scrape, and re-re-scrape) a Russian Harry Potter fanfic archive multiple times after not being careful enough: itâ€™s almost always better to scrape <em>too much stuff</em> and have to do some cleaning later, than to <em>not scrape enough stuff</em> and have to re-do it, especially if you donâ€™t notice until after youâ€™ve spent a lot of time cleaning up Ghost Cat data-hairballs.</p>

<p>What to do? Well, <code class="highlighter-rouge">:nth-of-type(n+3)</code> is a modifier on <em>ul</em>, so you can drop it to get ALL THE UNORDERED LISTS. But actually, you can go one step further and even get rid of the <em>ul</em> part, because what youâ€™re actually grabbing <em>isnâ€™</em>t unordered lists, but <em>links</em> (in HTML, &lt;a&gt; for anchor) that have the class <code class="highlighter-rouge">category-page__member-link</code>. The period after the a in the selector indicates a class attribute, which you can see if you switch from the â€œWeb Scraperâ€ tab to the â€œElementsâ€ tab in the developer toolbar.</p>

<p><img src="/site/assets/dscm3_html_view.jpg" alt="HTML page view" /></p>

<p>Each link corresponds to HTML that looks something like: <code class="highlighter-rouge">&lt;a href="/wiki/Abby_and_the_Best_Kid_Ever" title="Abby and the Best Kid Ever" class="category-page__member-link"&gt;Abby and the Best Kid Ever&lt;/a&gt;</code>. The text <em>Abby and the Best Kid Ever</em> (the one that occurs between <code class="highlighter-rouge">&gt;</code> and <code class="highlighter-rouge">&lt;</code>) is the text that actually appears on the page. The rest is the HTML instructions used to make that text into a link. But not just any link: a link with the class category-page__member-link, which <em>only</em> appears with links that are part of this list of books in the series corresponding to this wiki page (Regular, Mystery, etc.) Those are the links you want. To select &lt;a&gt; tags with the class category-page_member-link, click in the selector text box that reads ul:nth-of-type(n+3) a.category-page_member-link, and delete everything before the <em>a</em>.</p>

<p>One good way to make sure youâ€™re generally getting what you want is to click the â€œData previewâ€ button once you have something in the selection box. What you should see is all the URLs and link titles on the page, along with a _<em>followSelectorId</em> field that has the value of pagelink for all the links (i.e. the name you gave the link selector when you created it). What we actually <em>need</em> is just the URLs, so the rest is Ghost Cat hairball, but thereâ€™s no easy way to get <em>only</em> what we need using the Webscraper.io plugin, so weâ€™ll take it all and clean it up later.</p>

<p><img src="/site/assets/dscm3_data_preview.png" alt="Previewing data" /></p>

<p>Click the blue <em>Save selector</em> button at the bottom of the interface.</p>

<h4 id="scraping-and-exporting">Scraping and exporting</h4>

<p>Now itâ€™s time to do the scraping. In the webscraper.io interface, go to <em>Sitemap bsc_fan_wiki_link_scraper &gt; Scrape</em>. The window that pops up has two options: Request interval and Page load delay. Request interval means how long to wait before asking the websiteâ€™s server for a new page, assuming thereâ€™s more than one page. The thing is, if you hammer a server with requests, youâ€™re asking to get throttled: having your requests slowedâ€¦. way â€¦ waaayyyyyyyyâ€¦. down. Itâ€™s the price you pay for not having good web scraping manners. 5 seconds (5000 ms) is probably a good place to start, but if youâ€™re doing scraping at scale (tens or hundreds of pages), you still might be throttled as a bot (which, letâ€™s face it, you kinda are) if your delay is the same every time. (We can do more sophisticated things by writing a scraper in Python that can get around some of those issues, but thatâ€™s a guide for another DSC book.) In this case, though, where we only have a few pages to scrape, the default is fine. As for <em>Page load delay</em>, if you have a slow connection, or are scraping a complex site that takes some time to load, you may want to increase this value. If you want to guesstimate how much it should be, time a few page loads (from the new page appearing on your screen, to everything being loaded) by pulling up pages in your browser.</p>

<p>Click the blue â€œStart scrapingâ€ button. A new browser window will pop open with the page(s) that youâ€™re scraping; donâ€™t close it, itâ€™ll close itself when itâ€™s done.</p>

<p>When the window closes itself, itâ€™ll take you to a page that says â€œNo data scrapedâ€; hit the blue â€œRefreshâ€ button and your data will appear. Then go to <em>Sitemap bsc_fan_wiki_link_scraper &gt;  Export data as CSV</em>. On that page, click the <em>Download now!</em> link, and youâ€™ll download a CSV file.</p>

<p><img src="/site/assets/dscm3_download_csv.png" alt="Downloading a CSV" /></p>

<p>If you want to edit your scraper to include multiple pages, go to <em>Sitemap bsc_fan_wiki_link_scraper &gt; Edit metadata</em>. Thereâ€™s a + button on the right side of the URL field, and you can use it to put in more than one page (e.g. adding the URLs for <a href="https://babysittersclub.fandom.com/wiki/Category:Mystery_books">Mysteries</a>, <a href="https://babysittersclub.fandom.com/wiki/Category:Super_Special_books">Super Specials</a>, <a href="https://babysittersclub.fandom.com/wiki/Category:Super_mysteries">Super Mysteries</a>, <a href="https://babysittersclub.fandom.com/wiki/Category:Portrait_Collection_books">Portrait Collection</a>, <a href="https://babysittersclub.fandom.com/wiki/Category:California_Diaries_series">California Diaries</a>, <a href="https://babysittersclub.fandom.com/wiki/Category:Readers%27_Request_books">Reader Requests</a>, and <a href="https://babysittersclub.fandom.com/wiki/Category:Friends_Forever_series">Friends Forever</a>). Then, just re-scrape and re-download.</p>

<h4 id="just-the-urls">Just the URLs</h4>

<p>We could import this CSV into OpenRefine, but honestly, itâ€™s faster and easier to pull the CSV into Google Docs or Excel, delete the stray rows (e.g. Category:Jessi books), and copy only the <em>pagelink-href</em> column (which has the URLs). At this scale (fewer than 300 rows), it probably makes the most sense to just skim and do this cleanup manually, but if you search for â€œCategoryâ€ (which will give you links that just take you to sub-categories, like â€œMallory Booksâ€), and â€œKarenâ€ (which is a giveaway for books in the â€œBaby-Sitterâ€™s Little Sisterâ€ book series, about Kristyâ€™s super-annoying step-sister Karen), that should help you flag the bigger sets of bad results. Thereâ€™s also random pages that accidentally got mis-tagged (e.g. at least as of when Iâ€™m writing this, â€œHow to cosplay as Stacey McGill from the BSCâ€), so you do actually need to read through the list with your eyeballs, and check on any page titles where youâ€™re unsure. You should also de-duplicate links as needed, so that you only have one copy of each link (e.g. â€œStacey and the Haunted Masqueradeâ€ was tagged with both the regular series and mystery, so it appears twice.) Sorting the URLs alphabetically should help make these visible, and most spreadsheet software can easily replace duplicates automatically. (If youâ€™re using Google Sheets, you can just go to <em>Data &gt; Remove</em> duplicates to take care of it.)</p>

<p>All together, you should have 229 links to scrape.</p>

<h4 id="scraping-all-the-things">Scraping ALL THE THINGS</h4>

<p>You <em>could</em> load those 229 pages into Webscraper.io by hitting the + button on the URL field 228 times, and no doubt you may be tempted, especially if youâ€™re delegating this work to a research assistant. But resist the tantalizing purr of the Ghost Cat leading you down the path to rote labor: thereâ€™s an easier way.</p>

<p>What you need is a web page with all the links you want to scrape (as HTML links, not just plain text). The easiest way to get there, as of March 2020, is to use a site like <a href="https://pastelink.net/">pastelink.net</a>, where you can paste the URLs from the spreadsheet you cleaned up in the last step, and itâ€™ll give you a link that you can use for Webscraper.io. The site has been around since at least 2015, though it seems at least moderately sketchy (their â€œnews and updatesâ€ page mentions a successful purge of child abuse links in 2018 ğŸ˜¬), and it canâ€™t handle URLs that have an exclamation mark at the end (as more than a couple BSC books do â€“ along with their corresponding wiki pages), but itâ€™s your easiest option if youâ€™re not comfortable making an HTML or Markdown page.</p>

<p>A less-dodgy alternative, if you have a GitHub account and are comfortable with Markdown or HTML, is to create a file in a GitHub repo as a place to put the URLs. This approach makes it possible to make all the URLs work, even if they end in an exclamation mark. To make a <a href="https://github.com/datasittersclub/dscm3/blob/master/bsc_wiki_links.md">Markdown file that you can put on GitHub (like weâ€™ve done here)</a>, try this: if you copy and paste the list of URLs, one URL per line into a plain-text editor that supports regular expressions (which you can <a href="https://programminghistorian.org/en/lessons/understanding-regular-expressions">read up on at The Programming Historian</a> â€“ theyâ€™re basically a fancy find-and-replace syntax), you can search for <code class="highlighter-rouge">^(.*)$</code> (which translates to â€œgrab all the characters from the beginning of the line to the end of the lineâ€) and replace it with <code class="highlighter-rouge">[$1]($1)</code> (â€œput what you grabbed between square brackets, then between parenthesesâ€), though you may need to check your text editor documentation for whether its flavor of regular expressions uses $1 or \1 or some other notation for captured groups. If you get this to work, save the results as a .md file, and push that file to a GitHub repo, you can use the URL for that Markdown file in the next step (just like weâ€™ve done, <a href="https://github.com/datasittersclub/dscm3/blob/master/bsc_wiki_links.md">using our wiki links Markdown file</a> in our <a href="https://github.com/datasittersclub/dscm3/blob/master/bsc_book_metadata.txt">book metadata scraper</a>).</p>

<p>However you put your list of links online, the next step is to create a new sitemap in webscraper.io, and put the URL <em>of the page with all your links</em>, not any of the individual links. Then, the first selector in the sitemap that you create should be set to have multiple values, and what youâ€™re selecting for is each of the links on the page.</p>

<p><img src="/site/assets/dscm3_linkselector.jpg" alt="Selecting all the links" /></p>

<p>Once youâ€™ve saved that selector, click on it in your sitemap. This takes you <em>inside</em> that selector, as Webscraper.io structures things. The new selectors you add arenâ€™t selectors for your page of links, theyâ€™re selectors for the <em>pages whose URLs are on that page of links</em>. For that reason, itâ€™s going to be very helpful if the links in a single list go to pages that are pretty homogenous: you want the nested selectors to be applicable to most or all of those pages. Not all the selectors I created here work for every book (e.g. the California Diaries and Portrait books donâ€™t really have book numbers), but most of the fields are relevant across the board.</p>

<p>As an important side note here, wikis hosted on fandom.com are <em>unusually well-structured</em> and pretty remarkable for their use of semantically-inflected markup (i.e. a lot of the HTML elements include a class â€“ as weâ€™ve seen above â€“ that tells you something about the content that youâ€™ll find inside). Compare, even, to Wikipedia: even though you have what looks like structured metadata in the sidebar boxes for most articles, if you inspect the HTML, youâ€™ll find a table with the class â€œinfoboxâ€, and then a bunch of table rows (&lt;tr&gt;) and cells (&lt;td&gt;). What if you were reading the news and wanted to, say, set up a scraper that would go to the Wikipedia page for a bunch of different historical pandemics and extract the death toll? Good luck!</p>

<p>Probably the best you could do is capture the whole sidebar table, and try to (as likely as not, manually) extract the number youâ€™re looking for. Having an HTML class that says what it is (e.g. class=â€death-tollâ€ on a &lt;tr&gt; element) isnâ€™t the only setup that would let you scrape the data easily; if every pandemic page had a 100% consistent set of metadata, which was always provided in the same order (and listed as â€œN/Aâ€ or something if there was no known value), you might be able to reliably use something like <code class="highlighter-rouge">tr:nth-of-type(n+3)</code> in your scraper if you could count on the death toll always being the third element in that sidebar. But in reality, very few websites use either totally consistent metadata, or semantic classes on their elements. In most cases, for the results to be usable, web scraping is just a prelude to lots and lots of data cleaning.</p>

<p><img src="/site/assets/dscm3_fake_metadata_structure.png" alt="Wikipedia's fake metadata structure" /></p>

<p>But fandom.com wikis make our lives easy! Hereâ€™s the elements I created for all the data I wanted to capture; all of them are <em>Text</em> type and only <em>wikipagetext</em> (the body text on the wiki page) has the â€œmultipleâ€ boxes selected, because thereâ€™s usually multiple paragraphs of text on a wiki page:</p>

<table>
  <thead>
    <tr>
      <th>Element name</th>
      <th>Selector</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>booktitle</td>
      <td>h1.page-header__title</td>
    </tr>
    <tr>
      <td>bookseries</td>
      <td>[data-source=â€™seriesâ€™] div</td>
    </tr>
    <tr>
      <td>booknumber</td>
      <td>[data-source=â€™no. in seriesâ€™] div</td>
    </tr>
    <tr>
      <td>bookauthor</td>
      <td>[data-source=â€™authorâ€™] div</td>
    </tr>
    <tr>
      <td>bookcoverart</td>
      <td>[data-source=â€™illustratorâ€™] div</td>
    </tr>
    <tr>
      <td>booktagline</td>
      <td>[data-source=â€™taglineâ€™] div</td>
    </tr>
    <tr>
      <td>publication</td>
      <td>[data-source=â€™dateâ€™] div</td>
    </tr>
    <tr>
      <td>wikipagetext</td>
      <td>.mw-content-ltr &gt; p</td>
    </tr>
  </tbody>
</table>

<p>Now, if you want to make your life easier, omit the <em>wikipagetext</em> element. What I initially wanted to get out of it was the â€œDear Readerâ€ section for the books that have it (e.g. what is the wholesome, relatable message at the end of <em><a href="https://babysittersclub.fandom.com/wiki/Stacey%27s_Big_Crush">Staceyâ€™s Big Crush</a></em>, where she has a crush on a student teacher?), but thereâ€™s no HTML class to indicate that section, and itâ€™s not like the book synopses are reliably 5 paragraphs long, so that you could count on â€œDear Readerâ€ being the 6th. But because this element has multiple values, and each one gets saved as its own row in the output file, itâ€™s going to be a pretty gross data-hairball.</p>

<p>Once youâ€™ve set this up, run the scraper, download the results, and youâ€™ve got a brand new Ghost Cat data-hairball to clean up in OpenRefineâ€¦ in addition to all those records from the various national libraries.</p>

<h3 id="got-ghost-cat-data-hairballs-call-openrefine">Got Ghost Cat Data-Hairballs? Call OpenRefine.</h3>

<p>When you need to clean up metaphorical data-hairballs from an imaginary Ghost Cat, you need a tool with a fairytale story. <a href="https://openrefine.org/">OpenRefine</a> started life as Freebase Gridworks, became Google Refine when its parent company was acquired, and then when Google stopped supporting it, it became an open-source project called OpenRefine, and, in 2019, got $200,000 from the Chan Zuckerberg (yeah, <em><a href="https://en.wikipedia.org/wiki/Mark_Zuckerberg">that Zuckerberg</a></em>) Initiative to fund its development in 2020 as â€œEssential Open Source Software for Scienceâ€. And while they donâ€™t use a Humanities-inclusive Wissenschaft definition of â€œscienceâ€, Iâ€™m inclined to agree: OpenRefine has been essential for a lot of my computational text analysis work.</p>

<p>Download and install the software as you usually would, <a href="https://openrefine.org/">from their website</a>. If youâ€™re on a Mac, youâ€™ll have to deal with the â€œUnidentified developerâ€ warning message; just Ctrl+click on the OpenRefine icon and open it from the menu that appears in order to bypass that.</p>

<p>For the Ghost Cat data-hairballs weâ€™re working with here, the default settings for OpenRefine should be fine, but once you get into the thousands or hundreds of thousands of records, using OpenRefine will be torture if you donâ€™t <a href="https://github.com/OpenRefine/OpenRefine/wiki/FAQ:-Allocate-More-Memory">increase the softwareâ€™s memory allocation</a>.</p>

<p>One funny thing about OpenRefine is that you launch it as an application, but what it does is open a browser tab. At least once a week, I can see the OpenRefine diamond icon among my open applications, but for the life of me I canâ€™t find the tab itâ€™s running in â€“ or maybe I closed it â€“ so I shut it down and restart it to launch a new tab. When it launches, the tab it opens takes you to an interface for creating a new project by importing a file. Letâ€™s start by uploading the file we just downloaded from webscraper.io, with all the information from the BSC fan wiki.</p>

<p>After you upload it, thereâ€™s a screen where you can configure the import. It makes its best guesses, but those can be hit or miss. If yours looks wonky, make sure that the option is selected for columns to be separated by commas:</p>

<p><img src="/site/assets/dscm3_import_csv.png" alt="Importing a CSV into OpenRefine" /></p>

<p>Once youâ€™re happy with the preview, click the â€œCreate projectâ€ button in the upper right. This takes us to the regular OpenRefine screen.</p>

<h4 id="step-1-getting-one-row-per-book-from-the-fan-wiki-data">Step 1: Getting One Row Per Book from the Fan Wiki Data</h4>

<p>You may notice that the lines in the file that you uploaded seem to be in a random order: webscraper.io doesnâ€™t scrape in the order you might expect, in alphabetical order like you specified. Weâ€™ll start by sorting by the URL of each page you scraped data from. Click the downward-facing arrow next to the <em>bookurl</em> column header, and choose â€œSortâ€ then â€œOKâ€ in the box that pops up. Now your results are clustering by the book theyâ€™re associated with (starting with <em>Abby and the Best Kid Ever</em>), but if you look at the third column from the left, you can see that this view is just a display for your benefit: the data itself hasnâ€™t been re-sorted. We can change that by clicking on the down arrow next to Sort above the cells, and choosing â€œReorder rows permanentlyâ€. This changes the row numbers associated with <em>Abby and the Best Kid Ever</em> to 1-7.</p>

<p><img src="/site/assets/dscm3_reorder.png" alt="Reordering rows in OpenRefine" /></p>

<p>Having the data permanently reordered this way makes it possible for us to clean up one part of this hairball: the fact that thereâ€™s a row for every single paragraph of text on the wiki page, and all of those rows are identical except for the initial web-scraper-order column and the final <em>wikipagetext</em> column. What we want here is to merge all of those paragraph-cells into a single cell, with a single row for each book.</p>

<p>First, delete the <em>web-scraper-order</em> column by hitting the arrow to the left of its name and choosing <em>Edit column &gt; Remove column</em>. You can also delete the* web-scraper-start-url* column, which is the same for all the rows, and the <em>bookurl-href</em> which should be redundant with <em>bookurl</em>.</p>

<p>Next, what we need to do is called â€œBlank downâ€. When you have a bunch of rows next to one another that repeat data in almost all of their cells, blank down deletes the repetitions. Click on the arrow to the left of <em>every column name</em> except for <em>wikipagetext</em>, and choose <em>Edit cells &gt; Blank down</em>. The first instance of each value in a column will stay; every subsequent one will be removed. (Note: this only works if the rows are ordered so that repeated column values are all grouped together.)</p>

<p>When youâ€™re done, you should have a completely filled-in row, followed by a set of rows that are blank except for the <em>wikipagetext</em> column, followed by another completely filled-in row for another book â€“ and so on. Now for the magic: click the arrow to the left of the <em>wikipagetext</em> header and choose <em>Edit cells &gt; Join multi-valued cells</em>. A window will pop up where you can select the separator; replace the default comma with a space. This will get you the output youâ€™re looking for: a single row per book, with the full text from the wiki page in the final cell.</p>

<p>What weâ€™re ultimately going to want to do is add columns to the wiki spreadsheet that we just cleaned up, for information from the data that Lee gathered from the national libraries. But before we combine the national library data with the wiki data, we need to clean up the national library data.</p>

<h4 id="step-2-clean-up-quebec-data">Step 2. Clean up Quebec data</h4>

<p>What Lee sent me from Quebec was a series of CSVs, one per year (1990-1996). Thereâ€™s lots of ways you can combine those files, the most obviousâ€“ if rather inefficientâ€“ method being copying and pasting the content (minus the headers) from all the files into a single file. However you choose to do it, load the resulting CSV file into OpenRefine as a new project. If the default settings donâ€™t pick up the use of a semicolon as the separator, youâ€™ll need to manually configure that under â€œColumns are separated byâ€ by choosing â€œCustom separatorâ€ and putting in a semicolon. Youâ€™ll also need to uncheck â€œParse next 1 line(s) as column headersâ€. Then click â€œCreate projectâ€, in the upper right of the interface.</p>

<p>The first record (â€œBienvenue, Marjorie!â€, if youâ€™re using the data file in our GitHub repo) will be a little messed up â€“ split between two rows. The fastest way to fix this is to copy and paste the values into the right places, then delete the erroneous second row. If you hover over a cell, a blue â€œeditâ€ button appears in the upper right of that cell, and you can use that to both copy and paste text.</p>

<p>Keeping an eye on the end goal of being able to sync up this spreadsheet with the wiki data, we need one column that has the unique ID for each book. In this case, itâ€™s column 5, which looks like â€œLes baby-sitters ; 15â€; what we need to do is replace the first part with nothing, leaving us just the number. Hit the arrow in the header for that column, then choose <em>Edit cells &gt; Replace</em> and replace â€œLes baby-sitters ; â€œ with nothing.</p>

<p>We also need to remove all the â€œBaby Sitterâ€™s Little Sisterâ€ books; click the arrow in the header for the same column that previously had â€œLes baby-sittersâ€, choose <em>Text filter</em>, and put in â€œLe Petit monde de Karenâ€; this gave me 6 rows, and I deleted them by hitting the arrow to the left of â€œAllâ€ (the column header furthest to the left), and choosing <em>Edit rows &gt; Remove matching rows</em>. This will give you a blank screen of results; delete the text filter in the left column by hitting the X, and youâ€™ll be back to the remaining rows.</p>

<p>Next, we need to normalize the series number for Super Specials, Mysteries, and Portrait books. Hit the arrow to the left of the column that previously had â€œLes baby-sittersâ€, and choose <em>Facet &gt; Text facet</em>. This will open a new facet on the left with every unique value in that field. Using that, we can see that all the Super-Special books start with â€œLes baby-sitters. Super spÃ©cial ; no â€œ and all the Mysteries start with â€œLes baby-sitters. MystÃ¨re ; no â€œ. We can do the same thing â€“ <em>Edit cells &gt; Replace</em> â€“ and replace the text associated with Super-Specials with â€œSSâ€ and the Mystery text with â€œMâ€. (Youâ€™ll have to do this twice for the Super-Specials, because the data isnâ€™t completely consistent. All but one use â€œ<em>Les baby-sitters. Super spÃ©cial ; no</em> â€œ and one omits the â€œnoâ€.) Thereâ€™s one â€œLes baby-sitters. Collection Portraitâ€; just manually change it to â€œP2â€ (Claudiaâ€™s book was the second one published). If you hover over the text in the facet, youâ€™ll see an â€œeditâ€ option pop up, and you can edit the text directly from the facets.</p>

<p>Letâ€™s revisit the list of things we want out of this data:</p>

<ol>
  <li>The <em>translated title</em> of the book: we have this in column 1, before the /</li>
  <li>A <em>unique identifier</em> to connect back to the original book (book number, title, etc.): we have this now in column 5 after doing the clean-up</li>
  <li><em>Which translation series is it in</em> (e.g. for French: Quebec, Belgian, or France): we know this because of the source of the data</li>
  <li><em>The date it was published</em>: this is in column 3, but we need to extract it from the other data there</li>
  <li><em>The translator</em>: we have this in column 8, but we should clean it up</li>
</ol>

<p>To get the data we want for the translated title, use the <em>Edit Cells &gt; Replace</em> function for column 1, and check the â€œRegular expressionâ€ box. In the first box, put:Â  <code class="highlighter-rouge">(.*) \/ (.*)</code> and in the second, <code class="highlighter-rouge">$1</code>. This looks for all characters (letters, numbers, etc.) before a slash, and all the characters after a slash, and replaces all the text with just the characters before the slash. (Yeah, I had to double-check that with a regular expression tester before I used it.)</p>

<p>For the publication date, we need to do the same thing in column 3, but with <code class="highlighter-rouge">(.*)([0-9]{4})(.*)</code> in the first box (which looks for the text before the occurrence of 4 numbers, the 4 numbers, and after the 4 numbers), and <code class="highlighter-rouge">$2</code> in the second (which replaces the text with just the numbers, which in this case represents a date.) Donâ€™t forget to check the â€œRegular expressionâ€ box or it wonâ€™t work!</p>

<p>For the translator (column 8), use <em>Edit Cells &gt; Replace</em> and replace ` , trad.` with nothing.</p>

<p>Now, we can delete every column except for 1, 3, 5, and 8.</p>

<p><img src="/site/assets/dscm3_missing_quebec_value.png" alt="Missing value in Quebec data" /></p>

<p>Notice anything? Weâ€™ve caught a metadata inconsistency on the part of some librarians in Montreal! The translator column is empty for <em>Les baby-sitters en vacances dâ€™hiver</em>! Is the data missing, or just misplaced? By going back to the original file that Lee downloaded, I was able to find that the info was present in column 1 before we cleaned it up: the person who â€œadapted it from the Americanâ€ was Sylvie Prieur. So letâ€™s fill that in manually in the translator column before moving on.</p>

<p>With the data all cleaned up like this, we can go to <em>Facet &gt; Text facet</em> for the translator column to discover that, according to the data we have, there were 4 Quebec translators: Nicole Ferron did the most (24), Marie-Claude Favreau and Sylvie Prieur each did 16, and Lucie Duchesne did 12. If we choose <em>Text filter</em> for the column that has the book numbers, and put in â€œSSâ€ as the value, we see that no one seemed to specialize in the super-specials, but if we put in â€œMâ€, we find that almost half of Lucie Duchesneâ€™s contributions (5 out of 12) were in the mystery sub-series.</p>

<h4 id="step-3-clean-up-the-data-from-the-french-national-library">Step 3. Clean up the data from the French National Library</h4>

<p>The data Lee exported from the French National Library includes both the books published in France, and the Belgian translations. When you import the French metadata, OpenRefine parses the file correctly by default, and you can go ahead and create the project.</p>

<p>The French metadata includes a record for every copy the library has, so thereâ€™s duplicates to eliminate first. Sort the records by the second column, â€œnÂ° notice BnFâ€, then in that column, go to <em>Edit cells &gt; Blank down</em>. For that column again, go to <em>Facet &gt; Customized facets &gt; Facet by blank</em>. In the filter on the left, choose â€œtrueâ€ (i.e. blank). Then, when youâ€™re viewing only the rows that are blank click the arrow next to â€œAllâ€ and go to <em>Edit rows &gt; Remove matching rows</em>.</p>

<p>To make it simpler to get to see data that we want, letâ€™s delete some unnecessary columns: <em>Type de notice, Type de document, Localisation, Exemplaire nÂ°, Sujet, Couverture, Langue, Format</em>, and the blank final column.</p>

<p>It looks like the <em>Auteur</em> column has the translator information. We could try to get to it by splitting the values in that column (by using <em>Edit column &gt; Split into several columns</em>, using the | character as the separator), but if you start down that road, youâ€™ll discover that sometimes the translator doesnâ€™t get marked in that field, but the translator seems to be more reliably available through the Titre column. Also, the Auteur column doesnâ€™t differentiate contributor role (e.g. translator vs. illustrator), and youâ€™d need to cross-check with the <em>Titre</em> column anyway to identify the correct translator. So letâ€™s work from the <em>Titre</em> column instead.</p>

<p>For the <em>Titre</em> column, go to <em>Edit column &gt; Split into several columns</em> using the semicolon as the separator. The resulting â€œTitre 1â€ will have the book column and (in most cases) Ann M. Martin. â€œTitre 2â€ mostly has translators, with a few illustrators and other kinds of authors (e.g. â€œa comic strip by Raina Telgemeierâ€). Only one row in â€œTitre 3â€ and two rows in â€œTitre 4â€ have a translator. For each of these columns, delete the values that donâ€™t include a translator. The easiest way to do this might be to do <em>Facet &gt; Text facet</em> for each column, and edit the ones that arenâ€™t a translator (by hovering over the value in the sidebar facet, and clicking â€œeditâ€, and deleting the value). Once <em>Titre 2-4</em> are all just translators, go to <em>Titre 2</em> and go to <em>Edit column &gt; Join columns</em>. Check the boxes for <em>Titre 3</em> and <em>Titre 4</em>, and choose the semicolon as the separator. Once youâ€™ve merged all values into <em>Titre 2</em>, you can delete Titre 3-4. Next, itâ€™s time to clean up <em>Titre 2</em>, which now has all the translator information. Do <em>Facet &gt; Text facet</em> and use <em>Edit cells &gt; Replace</em> to replace all the variations on â€œtranslated byâ€ (e.g. â€œtrad. de lâ€™anglais parâ€, â€œtrad. de lâ€™amÃ©ricain parâ€, â€œtraduit de lâ€™anglais (Ã‰tats-Unis) parâ€) along with extraneous punctuation like â€œ]; ;â€ until the values in that field coalesce (i.e. until you have more rows sharing the same translator value, without extraneous variation like how â€œtranslated byâ€ was written). Another way to reduce variation is by going to <em>Edit cells &gt; Common transforms &gt; Trim leading and trailing whitespace</em>. One other check you can do is go to <em>Edit cells &gt; Cluster and edit</em>. Under the â€œmethodâ€ drop-down, choose â€œnearest neighborâ€. This may turn up discrepancies, for instance, like the addition of an extra â€œlâ€ to Camille Weilâ€™s name in one record:</p>

<p><img src="/site/assets/dscm3_cluster_and_edit.png" alt="Clustering values" /></p>

<p>Check the box for â€œMergeâ€, then choose â€œMerge selected and closeâ€.</p>

<p>How close are we now? Go to <em>Titre 2</em> and do <em>Facet &gt; Customized facet &gt; Facet by blank</em>.</p>

<p>Oof. 42 books without a translator listed. 11 Belgian books (if you do a text facet on Editeur, the Belgian ones are â€œChantecler (Aartselaar)â€), 31 France books.</p>

<p>Can we salvage this by looking for the data in other columns? If you add a text filter on the Contributeur column and search for â€œtradâ€, thereâ€™s 2 rows that have (multiple) translators in the <em>Contributeur</em> column, who didnâ€™t appear in the Titre column, for some of the text compilations (i.e. books that contain multiple original books). Unfortunately, you canâ€™t tell from this how to map the translators to the individual books, so for our purposes, metadata from text compilations is pretty useless.</p>

<p>When I get stuck on one aspect of data cleaning, sometimes it helps to put it down for a bit and work on another part of the file. We know that the file that weâ€™re working with is everything by Ann M. Martin in the French National Library, so thereâ€™s non-BSC stuff in here too. The series information seems to be in the <em>Description</em> field, where it exists. Letâ€™s do a <em>Text filter</em> there to search for â€œLa petite soeur des baby-sittersâ€ (the Belgian â€œBaby-Sitterâ€™s Little Sisterâ€), then remove all matching rows. What we really want is to remove all non-BSC books, and then extract the series number from the <em>Description</em> field. But looking more closely through the data, not all the records have the series number, which is going to make a mess when we try to merge this data with our original English metadata. This is the moment where we realize that this Ghost Cat is also having digestive problems, in addition to its hairballs. Weâ€™ve got to lock it in the bathroom before weâ€™re left scrubbing down the whole house.</p>

<p>So what gives? Is the data actually not there? Or is it just not in the download that Lee got?</p>

<p>The <em>Identificat</em> column contains the URL of the catalog record, along with the ISBN number. The ISBN number doesnâ€™t help us any, so letâ€™s go to <em>Edit cells &gt; Replace</em>, check the box for regular expression, and put in ` .*`, which will select everything that occurs after the first space (which appears after the catalog record URL. Replace with nothing. This should get you valid, clickable URLs for each of the entries.\
The records where the description starts with â€œCollection : (Folio juniorâ€ are our problem: no series number, and no other way to try to link back to the original English (like the original English title). Letâ€™s check out <a href="https://catalogue.bnf.fr/ark:/12148/cb367080517">Mallory entre en scÃ¨ne</a>â€¦ and there it is. Weâ€™ve got both â€œTraduction de: Hello, Malloryâ€ and â€œTitre dâ€™ensemble : (Le club des baby-sitters. ; 14)â€ in the catalog record.</p>

<p><img src="/site/assets/dscm3_bnf_catalog.png" alt="BNF catalog record" /></p>

<p>But if you try to export search results from the catalog, those fields arenâ€™t available.</p>

<p><img src="/site/assets/dscm3_public_export.png" alt="BNF catalog export" /></p>

<p>But wait! What about that <em>paramÃ©trage professionnel</em> tab?</p>

<p><img src="/site/assets/dsc_m3_professional_export.png" alt="BNF professional catalog export" /></p>

<p>Intermarc? Unimarc? I know about the <a href="https://en.wikipedia.org/wiki/MARC_standards">MARC standards</a> for library cataloging, and I have a piece of paper lying around here saying that Iâ€™m officially certified as a Library and Information Scientist, but â€¦ ugh.</p>

<p>But then I thought about Arcadia Falcone, the metadata librarian at work and one of the most quirky-cool people I know. Sheâ€™d probably know the MARC codes I needed for these records off the top of her head, and could figure out Unimarc with both hands busy crocheting an amazing shawl. I could do this, too. And thanks to the Library of Congress, it wasnâ€™t actually too bad; thereâ€™s a â€œ<a href="https://www.loc.gov/marc/unimarctomarc21.html">UNIMARC to MARC 21 Conversion Specifications</a>â€ that was published only 9 months after the last Baby-Sitters Club book. The formatting alone suggests a promising treatment for insomniaâ€“ try it the next time nightmares wake you up in the middle of the night! (I canâ€™t be the only one with that problem these daysâ€¦)</p>

<p><img src="/site/assets/dscm3_unimarc_marc21.png" alt="Unimarc to MARC21" /></p>

<p>Searching for â€œtranslationâ€ scored me 453 for the field with the English title of the book, and searching for â€œseriesâ€ got me 225 for the field with the book number (even better than the English book title). I stuck â€œ225;â€ into the field, hit Exporter, and held my breath.</p>

<p>What I got was an ExportPro.csv file, with the fields â€œIdentifiant ARKâ€, â€œNÂ° notice BnFâ€, â€œType de noticeâ€, â€œType de document;=â€225â€â€. The spreadsheet Iâ€™d already partly cleaned in OpenRefine had that â€œNÂ° notice BnFâ€ field, too, as a unique value for each book â€“ all I needed to do was connect my new data with my old OpenRefine project using that â€œNÂ° notice BnFâ€ value to match everything up. Time for another metadata librarian to the rescue! <a href="https://ruthtillman.com/">Ruth Kitchin Tillman</a> recently did an <a href="https://ruthtillman.com/talk/cell-cross-webinar-2020-03/">online OpenRefine workshop on â€œcell crossâ€</a>, which is the OpenRefine jargon for doing exactly what we need to do here. Itâ€™s worth watching for a more detailed explanation, but uses a library cataloging example that might not be as relevant to Data-Sitters Club readers, so letâ€™s run through it here.</p>

<h4 id="look-both-ways-before-crossing-the-cells">Look both ways before crossing the cells</h4>

<p>The first step is to create a new OpenRefine project with our ExportPro.csv that has the series numbers. If you still have the earlier OpenRefine project from the French National Library open, you can open up a new OpenRefine tab by right-clicking on the logo in the upper left, and choosing â€œOpen link in new tabâ€. Import the ExportPro.csv file, and call the new project â€œExportProâ€; the defaults are mostly fine, but uncheck the â€œUse character â€œ to enclose cells containing column separatorsâ€ option. The column that we want to import has a name that might cause some errors (it includes quotation marks, which are often a problem in file and column naming). Rename the last column â€œvalue225â€.\
Now, go back to the earlier project with the French National Library data. Go to the â€œNÂ° notice BnFâ€ column and choose <em>Edit column &gt; Add column</em> based on this column.</p>

<p><img src="/site/assets/dscm3_cell_cross.png" alt="Adding a column for cell cross" /></p>

<p>In the box that pops up, weâ€™ll call the new column â€œseriesnumberâ€. The syntax here is a little tricky, but roughly follows the format <code class="highlighter-rouge">cell.cross(â€œOther-Projectâ€, â€œColumn-to-Match-in-Other-Projectâ€])[0].cells[Column-to-Import-from-Other-Project].value</code>. So in this case, the other project (the one we just created from ExportPro.csv) is called ExportPro, the column there that we want to match against is â€œNÂ° notice BnFâ€, and the column weâ€™re trying to import is called â€œvalue225â€: <code class="highlighter-rouge">cell.cross("ExportPro", "NÂ° notice BnF")[0].cells["value225"].value</code>.</p>

<p><img src="/site/assets/dscm3_cell_cross_detail.png" alt="Cell cross data preview" /></p>

<p>Success! Once youâ€™ve finished the cell cross, you can close the ExportPro OpenRefine project. All we need to do now is clean up the â€œseriesnumberâ€ field. Itâ€™s easiest to start by doing <em>Facet &gt; Text facet</em> on the â€œseriesnumberâ€ field, to get a sense of how much progress youâ€™re making in your cleaning. Replace <code class="highlighter-rouge">225$aLe club des baby-sitters.$v</code>, <code class="highlighter-rouge">225$aLe club des baby-sitters.$v</code>, <code class="highlighter-rouge">225$aLe club des babysitters$v</code>, and ` | 225$aFolio junior$v(.*)` (the broader â€œFolio juniorâ€ publication series) with nothing.</p>

<p>There are 18 records that just have the series â€œ225$aLe club des baby-sittersâ€; these are the text collections (â€œNos dossiers top-secretâ€, â€œNos plus belles histoires de coeurâ€, etc.) that have multiple books in one. Because we canâ€™t disambiguate who translated which of the sub-books, these records arenâ€™t very helpful for us; you can delete them all by choosing the facet â€œ225$aLe club des baby-sittersâ€, then going to the <em>All</em> column and choosing <em>Remove matching rows</em>. Do the same for the records with the â€œ225$aFolio juniorâ€ facet, which seem to represent series (like the mysteries, or comic books). You can also delete the record in the â€œ225$aFolio junior$v950â€ facet (not a BSC book), and the records with blank values for â€œseriesnumberâ€ (more series records or not BSC books). This leaves us with a set of mystery entries. Replace <code class="highlighter-rouge">225$aLe club des baby sitters mystÃ¨re.$v</code> and <code class="highlighter-rouge">225$aLe club des babysitters$iMystÃ¨re$v</code> with <code class="highlighter-rouge">M</code>.</p>

<p>Can we pause for a second and reflect on what it means to have to replace two different values for the mystery books, or three for the main series? Yeah, itâ€™s more work, and thatâ€™s kinda annoying, but when I see things like that, itâ€™s a reminder that these records were created by people (and more likely than not, <em>women</em>). Itâ€™s easy to treat a giant national library catalog with export functionality and millions upon millions of records as something digital, computerized, consistent â€“ but those records were originally created by people. People who might still be walking around in the world (or, at this present moment, hopefully sheltering-in-place at home). Who are they? What are their lives like? Do they have family members who are sick? Are they in a hospital, with a complex constellation of factors determining whether theyâ€™ll be one of the lucky ones who gets a ventilator? Think about that, next time youâ€™re using an online catalog to access digital resources. Real people made these records. And other real people did real work to convert them into the form that youâ€™re using. Pause for a moment to think of those people. And maybe send a thank-you note or cupcakes or flowers or something to your local libraryâ€™s technical services staff from time to time, once this is over.</p>

<h4 id="a-little-more-tidying">A little more tidying</h4>

<p>Letâ€™s finish cleaning up these BNF records and get ready to import them back into our master metadata spreadsheet. We just want the translated book titles in the â€œTitre 1â€ column, not Ann M. Martinâ€™s name, so replace ` / Ann M. Martin` with nothing.</p>

<p>We also need to make sure thereâ€™s only one instance of each â€œseriesnumberâ€ value, otherwise the cell cross wonâ€™t work. Go to <em>Facet &gt; Text facet</em> for â€œseries numberâ€, and sort by â€œcountâ€ at the top of the facet list.</p>

<p><img src="/site/assets/dscm3_seriesnumber_facets.png" alt="Series number facet" /></p>

<p>Uh-oh. We have <em>up to 5</em> instances of some of these series numbers. Letâ€™s dig into it.</p>

<p>Itâ€™s not shocking to get two copies: we have both Belgian and French data here. (Weâ€™ll need to export the Belgian data and set it up as its own project for a cell cross, but no big deal.)</p>

<p>What to do about the French data? Clicking through some examples, it looks like itâ€™s coming down to the question of reprints. Now, a book history of the Baby-Sitters Club in French would be fascinating! But remember what I saidâ€¦ umâ€¦ some 20 pages ago at this point? â€œSometimes the hardest part of DH is setting aside all the things you <em>could do</em>, to finish the thing that youâ€™re <em>actually doing</em>.â€ When I listed â€œpublication dateâ€ as one of the things I wanted to getâ€¦ well, given the choice of multiple dates, I want the first one. Reprints? You all gotta go.</p>

<p>I clicked through the facets; theyâ€™re already sorted in a way that works out to be chronological. Because the France-French versions were published after the Belgian ones (which didnâ€™t have reprints), we can go to the â€œEditeurâ€ column and choose *Edit cells &gt; Blank down**. This removes the â€œEditeurâ€ field for everything except the Belgian version and the first French version. Do this for each â€œseriesnumberâ€ facet (e.g. first click on 1, then on 2) with more than 2 values. (â€œBut Quinn,â€ you might object. â€œIsnâ€™t that rote work? Arenâ€™t you letting the Ghost Cat win? Couldnâ€™t you script that?â€ To which I would reply, â€œLook, itâ€™s fewer than 20 facets, Iâ€™m tired, letâ€™s just go clicky-clicky and be done with it, all right?â€) As youâ€™re doing it, though, pay attention to the book titles: if thereâ€™s anything that looks radically different than the other translated titles, Google it to make sure that the series number is right. Notably, â€œLa revanche de Carlaâ€ appears along with variations on â€œBienvenue, Marjorie!â€; it should be book 15, not 14. Itâ€™s also worth going a step further and checking that the facets with 2 values have them from different publishers; for instance, the BNF catalog has two records from France for book #4 â€œPas de panique, Mary Anneâ€, and no record for the Belgian equivalent â€œMÃ©lanie garde la tÃªte froideâ€.</p>

<p>Now, close the â€œseriesnumberâ€ facet and go to <em>Facet &gt; Text facet** for the â€œEditeurâ€ column. Click the â€œ(blank)â€ facet, and under *All</em>, choose <em>Edit rows &gt; Remove matching rows</em>. If you want to double-check your work, if you click either the â€œChantecler (Aartselaar)â€ or the â€œGallimard jeunesse (Paris)â€ facet, and add the â€œseriesnumberâ€ facet, each â€œseriesnumberâ€ facet should only appear with 1 value.</p>

<p><img src="/site/assets/dscm3_facets_clean.png" alt="Clean series number facet" /></p>

<p>Close the â€œseriesnumberâ€ facet, but keep the â€œEditeurâ€ facet open, and select â€œChantecler (Aartselaar)â€. In the upper right corner of the OpenRefine interface, go to <em>Export &gt; Comma-separated value</em>. This will give you a new CSV file with just the Belgian records; you may want to rename it <code class="highlighter-rouge">bnf_records_belgian.csv</code> just to keep things clear. With that data exported, you can delete the Belgian records from your current OpenRefine project by going to <em>All &gt; Edit rows &gt; Remove matching rows</em>.</p>

<p>Once again, create a new OpenRefine project, this time for the Belgian records you just exported. Import the CSV file you just exported (bnf_records_belgian.csv, if you renamed it). You shouldnâ€™t have to do any additional cleaning on it â€“ it should just be ready to go.</p>

<h3 id="step-4-revisiting-the-fan-wiki-data">Step 4. Revisiting the fan wiki data</h3>

<p>Before we combine the fan wiki data with the records from the national libraries, we need to do a little more cleanup. Go to <em>Facet &gt; Text facet</em> for the â€œbooknumberâ€ column. There shouldnâ€™t be more than one value for each of the facets, but some of them have multiple values (e.g. â€œKristyâ€™s Great Ideaâ€ â€“ the very first book â€“ and â€œBaby-sitters on Board!â€ â€“ the first Super Special book, both are listed under 1.) If you add a text facet for the â€œbookseriesâ€ column, you can do some bulk editing. For instance, if you select the â€œThe Baby-Sitters Club Mysteryâ€ facet under â€œbookseriesâ€, you can do <em>Edit cells &gt; Replace</em> and replace <code class="highlighter-rouge">^</code> with <code class="highlighter-rouge">M</code> (make sure you check the â€œregular expressionsâ€ checkbox) to put an M at the beginning of all the mystery books. Super-special books should similarly be prefixed with â€œSSâ€, Readerâ€™s Request should be â€œRRâ€, Super Mysteries are â€œSMâ€, Portrait Collection are â€œPCâ€, Friends Forever are â€œFFâ€, and California Diaries are â€œCDâ€. (We just made up these conventions, but they are how we actually name the files in our corpus.)</p>

<p>Once no facet other than â€œnullâ€ has more than one value, youâ€™re ready to bring in the data from the national libraries.</p>

<h3 id="step-5-the-ultimate-cell-cross-of-ultimate-destiny">Step 5. The Ultimate Cell-Cross of Ultimate Destiny<a href="#footnote">*</a></h3>

<p>Weâ€™ve got four OpenRefine projects open now:</p>

<ul>
  <li>Bsc_book_metadata: data scraped from the fan wiki</li>
  <li>Quebec_records: metadata for books published in Quebec</li>
  <li>Bnf_records: metadata for books published in France</li>
  <li>Bnf_records_belgian: metadata for books published in Belgium</li>
</ul>

<p>In Bsc_book_metadata, go to the â€œbooknumberâ€ column and choose <em>Edit column &gt; Add column based on this column</em>. Youâ€™ll need to do this once for each of the rows in the following table, which spells out what the new column should be called (which should be a good indicator of the data that youâ€™re pulling in), and the cell cross formula you need to get it.</p>

<p>You may see a lot of error messages in the previewed results â€“ thatâ€™s okay. We have over 200 rows for metadata scraped from the fan wiki, and only 80 rows from Quebec (and fewer still from Belgium and France). To increase the likelihood that youâ€™ll see results, you can re-sort the fan wiki data by the â€œbooknumberâ€ column, since earlier books were more likely to be translated.</p>

<p><img src="/site/assets/dscm3_ultimate_cell_cross_errors.png" alt="Cell cross missing values" /></p>

<table>
  <thead>
    <tr>
      <th>New column</th>
      <th>Cell cross formula</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>QuebecTitle</td>
      <td><code class="highlighter-rouge">cell.cross("quebec_records", "Column 5")[0].cells["Column 1"].value</code></td>
    </tr>
    <tr>
      <td>QuebecDate</td>
      <td><code class="highlighter-rouge">cell.cross("quebec_records", "Column 5")[0].cells["Column 3"].value</code></td>
    </tr>
    <tr>
      <td>QuebecTranslator</td>
      <td><code class="highlighter-rouge">cell.cross("quebec_records", "Column 5")[0].cells["Column 8"].value</code></td>
    </tr>
    <tr>
      <td>BelgiumTitle</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records_belgian", "seriesnumber")[0].cells["Titre 1"].value</code></td>
    </tr>
    <tr>
      <td>BelgiumDate</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records_belgian", "seriesnumber")[0].cells["Date"].value</code></td>
    </tr>
    <tr>
      <td>BelgiumTranslator</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records_belgian", "seriesnumber")[0].cells["Titre 2"].value</code></td>
    </tr>
    <tr>
      <td>FranceTitle</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records", "seriesnumber")[0].cells["Titre 1"].value</code></td>
    </tr>
    <tr>
      <td>FranceDate</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records", "seriesnumber")[0].cells["Date"].value</code></td>
    </tr>
    <tr>
      <td>FranceTranslator</td>
      <td><code class="highlighter-rouge">cell.cross("bnf_records", "seriesnumber")[0].cells["Titre 2"].value</code></td>
    </tr>
  </tbody>
</table>

<p>The last thing to do is to reorder the columns in a more sensible arrangement. Click the arrow next to â€œAllâ€, and choose <em>Edit columns &gt; Re-order/remove</em>. Weâ€™ll re-order the columns so all the French fields are at the end.</p>

<h3 id="where-to-from-here">Where to from here?</h3>

<p>Finally, we had our spreadsheet for English and French! All the books, all the ghostwriters, and many of the translators. The metaphorical Ghost Cat, curled up peacefully in our lap, purring. So much data brimming with possibility for answering new questions â€“ but what should those questions be?</p>

<p>As we were wrapping up writing this book, I asked Lee to take a look at the â€œDear Readerâ€ section.</p>

<blockquote>
  <p>From: Lee Skallerup Bessette</p>
</blockquote>

<blockquote>
  <p>To: Quinn Dombrowski</p>
</blockquote>

<blockquote>
  <p>Subject: Re: Dear Reader draft</p>
</blockquote>

<blockquote>
  <p>Date: 3/28/20 10:45 AM</p>
</blockquote>

<blockquote>
  <p>Yay! SOMETHING TO DO OTHER THAN LISTEN TO MY SON PLAY VIDEO GAMES AND MY DAUGHTER AND HUSBAND ARGUE.</p>
</blockquote>

<hr />

<blockquote>
  <p>From: Quinn Dombrowski</p>
</blockquote>

<blockquote>
  <p>To: Lee Skallerup Bessette</p>
</blockquote>

<blockquote>
  <p>Subject: Re: Dear Reader draft</p>
</blockquote>

<blockquote>
  <p>Date: 3/28/20 10:46 AM</p>
</blockquote>

<blockquote>
  <p>LOL! What do you want to do next?</p>
</blockquote>

<hr />

<blockquote>
  <p>From: Lee Skallerup Bessette</p>
</blockquote>

<blockquote>
  <p>To: Quinn Dombrowski</p>
</blockquote>

<blockquote>
  <p>Subject: Re: Dear Reader draft</p>
</blockquote>

<blockquote>
  <p>Date: 3/28/20 11:17 AM</p>
</blockquote>

<blockquote>
  <p>Werenâ€™t we going to move on to the translation/adaptations of the comic books? I mean, we can comic TEI the snot out of them, and maybe this isnâ€™t a mystery, but that sounds like a fun next step, at least on my end. You sent the French version(s) right? And the Quebec and France ones are the same, we think?</p>
</blockquote>

<blockquote>
  <p>How does that sound?</p>
</blockquote>

<hr />
<p>It sounded great. Weâ€™ll take a break from these translations for now, and start exploring the new form of The Baby-Sitters Club that young people these days are reading. Weâ€™ll be getting into TEI  encoding, methods for image comparison, and whatever else we stumble upon on the way. Weâ€™ll be back soon with more. Until then, take care of yourself, dear reader, and those around you.</p>

<p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<a name="footnote"></a>* Am I the only one whoâ€™s had strange, random music emerging from the corner of their brain after a couple weeks at home? I have no idea why this <a href="https://www.youtube.com/watch?v=4WgT9gy4zQA">12-year-old song</a> has gotten stuck in my head, but working on this final step, I canâ€™t help but re-imagine it, subbing out the Baby-Sitters Club, Sweet Valley High, American Girl Dolls, Cabbage Patch Kids, Rainbow Bright, My Little Ponies, and other differently-gendered references. Would Maria from Sesame Street emerge victorious â€“ perhaps by negotiating with the other iconic characters to work together instead of fighting, in order to ensure everyone has the resources they need to defeat COVID-19, from toilet paper to ventilators?</p>


        <footer class="site-footer">
  <h2 class="footer-heading">The Data-Sitters Club</h2>

  <div class="row">
    <div class="column">
      <ul class="contact-list">
        <li>Need a Data-Sitter? <br> Save time!<br> Call The Data-Sitters Club.
</li>
        <li><a href="mailto:"></a></li>
      </ul>
    </div>

    <div class="column">
      <ul class="social-media-list">
        
        <li>
          <a href="https://github.com/datasittersclub">
            <span class="icon  icon--github">
              <svg viewBox="0 0 16 16">
                <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>

            <span class="username">datasittersclub</span>
          </a>
        </li>
        

        
        <li>
          <a href="https://twitter.com/hashtag/datasittersclub?src=hashtag_click">
            <span class="icon  icon--twitter">
              <svg viewBox="0 0 16 16">
                <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>

            <span class="username">datasittersclub</span>
          </a>
        </li>
        
      </ul>
    </div>

    <div class="column">
      <p>A comprehensive, colloquial guide to digital humanities computational text analysis.</p>
    </div>
    <div class="column">
      <p>1st place, <a href="http://dhawards.org/dhawards2019/results/"><img src="/site/assets/DHAwards2019-fun.png" /></a> (tie)</p>
    </div>
<div class="column">
  <p>3rd place, <a href="http://dhawards.org/dhawards2019/results/"><img src="/site/assets/DHAwards2019-blog.png" /></a> (tie)</p>
		
</div>
  </div>

</footer>

      </div>
    
    </main>

    
    
  </body>

</html>
